# -*- coding: utf-8 -*-
"""
This script includes all MNIST GAN models.
> DCGAN
> LSGAN
> WGAN

Each GAN model is its own class object. Each class has its own attributes.
- init: initialises the object
- build_generator: builds the generator model
- build_discriminator: builds the discriminator model
- train: the training loop for the particular type of model
- sample_imgs: produces images generated by the generator
- save_weights: saves the weights of the model
- load_weights: loads saved weights into the model
- get_features: provides the output of the discriminator at the feature level. Input: Image.
"""
from __future__ import print_function, division

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, InputLayer, BatchNormalization, Activation, ZeroPadding2D, LeakyReLU
from tensorflow.keras.layers import UpSampling2D, Conv2D
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras import Input
from tensorflow.keras.metrics import binary_accuracy

import numpy as np
import time

class dcGanMnist():
    def __init__(self):
        """Initialises necessasry values and objects"""
        # Input shape
        self.img_rows = 28
        self.img_cols = 28
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 100
        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()
        # Build the generator
        self.generator = self.build_generator()

    def __repr__(self):
        description = "DCGAN model for MNIST data."
        return description

    def build_generator(self):
        """ Builds the generator"""
        model = Sequential()

        model.add(Dense(128*7*7, activation='relu', input_dim = self.latent_dim))
        model.add(Reshape((7, 7, 128)))
        model.add(UpSampling2D())
        model.add(Conv2D(128, kernel_size = 3, padding = "same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Activation("relu"))
        model.add(UpSampling2D())
        model.add(Conv2D(64, kernel_size=3, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Activation("relu"))
        model.add(Conv2D(self.channels, kernel_size=3, padding="same"))
        model.add(Activation("tanh"))
        assert model.output_shape == (None, self.img_cols, self.img_rows, self.channels)

        model.summary()

        return model

    def build_discriminator(self):
        """Builds the discriminator"""
        model = Sequential()

        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding="same"))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))
        model.add(ZeroPadding2D(padding=((0,1),(0,1))))
        model.add(BatchNormalization(momentum=0.8))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Flatten())
        #model.add(Dropout(0.25))
        model.add(Dense(1))

        model.summary()

        return model



    def train(self, epochs, BATCH_SIZE = 128):
        '''Trains the GAN model'''
        (X_train,_), (_,_) = mnist.load_data()
        X_train = X_train.reshape(X_train.shape[0], self.img_rows, self.img_cols, self.channels).astype('float32')
        X_train = (X_train - 127.5) / 127.5

        BUFFER_SIZE = 60000

        train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

        self.losses = np.zeros((epochs*len(train_dataset), 3))

        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)

        generator_optimizer = Adam(0.0001)
        discriminator_optimizer = Adam(0.0001)

        def discriminator_loss(real_output, fake_output):
            real_loss = cross_entropy(tf.ones_like(real_output), real_output)
            fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
            total_loss = real_loss + fake_loss
            return total_loss

        def generator_loss(fake_output):
            return cross_entropy(tf.ones_like(fake_output), fake_output)

        def discriminator_accuracy(real_output, fake_output):
            preds = np.concatenate([real_output, fake_output])
            target = np.concatenate([np.ones_like(real_output), np.zeros_like(fake_output)])
            preds_int = (preds > 0.5).astype(int)
            target_int = (target > 0.5).astype(int)
            ans = np.sum(preds_int == target_int)/len(preds)
            return ans

        @tf.function
        def train_step(images):
            noise = tf.random.normal([BATCH_SIZE, self.latent_dim])

            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                gen_imgs = self.generator(noise, training = True)

                real_output = self.discriminator(images, training = True)
                fake_output = self.discriminator(gen_imgs, training = True)

                gen_loss = generator_loss(fake_output)
                disc_loss = discriminator_loss(real_output, fake_output)

            grads_gen = gen_tape.gradient(gen_loss, self.generator.trainable_variables)
            grads_disc = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)

            generator_optimizer.apply_gradients(zip(grads_gen, self.generator.trainable_variables))
            discriminator_optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_variables))

            return disc_loss, gen_loss, real_output, fake_output


        for epoch in range(epochs):

            start = time.time()
            batch_size = len(train_dataset)
            batch = 0

            for image_batch in train_dataset:
                self.losses[batch_size*epoch + batch, 0] , self.losses[batch_size*epoch + batch, 1] , real_output, fake_output = train_step(image_batch)
                disc_acc = discriminator_accuracy(real_output.numpy(), fake_output.numpy())
                self.losses[batch_size*epoch + batch, 2] = disc_acc
                print("%d [D loss: %f] [G loss: %f] [D acc: %f]" % (batch_size*epoch + batch, self.losses[batch_size*epoch + batch, 0], self.losses[batch_size*epoch + batch, 1], self.losses[batch_size*epoch + batch, 2]))
                batch = batch + 1


            print("%d [Batch time: %.3f]" % (epoch, time.time()-start))

        print("Training complete!")



    def save_weights(self, name):
        """Saves models weights"""
        self.generator.save((name + "_dcgan_generator.h5"))
        self.discriminator.save((name +"_dcgan_discriminator.h5"))
        self.combined.save((name + "_dcgan_combined.h5"))

        return print("Model saved")

    def load_weights(self, name):
        """Loads model weights"""
        self.generator = load_model((name + "_dcgan_generator.h5"))
        self.discriminator = load_model((name +"_dcgan_discriminator.h5"))
        self.combined = load_model((name + "_dcgan_combined.h5"))

        return print("Model loaded")

    def get_feature_encoder(self):
        """Provides feature encoder from discriminator"""
        transformer = Model(self.discriminator.inputs, self.discriminator.layers[-2].output)

        return transformer

    def save_feature_encoder(self, name):
        """Saves weights of feature encoder"""
        encoder = self.get_feature_encoder()
        encoder.save((name + "_dcgan_encoder.h5"))

        return print("Encoder saved")

    def load_feature_encoder(self, name):
        """Loads weights of feature encoder"""
        encoder = self.get_feature_encoder()
        encoder = load_model((name + "_dcgan_encoder.h5"))
        encoder.compile()

        return encoder

    def get_training_metrics(self):
        """Provides training metrics"""

        return self.losses


class lsGanMnist():
    def __init__(self):
        """Initialises necessasry values and objects"""
        # Input shape
        self.img_rows = 28
        self.img_cols = 28
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 100

        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()

        # Build the generator
        self.generator = self.build_generator()

    def build_generator(self):

        model = Sequential()

        model.add(Dense(256, input_dim=self.latent_dim))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(np.prod(self.img_shape), activation='tanh'))
        model.add(Reshape(self.img_shape))
        assert model.output_shape == (None, self.img_cols, self.img_rows, self.channels)
        model.summary()

        return model

    def build_discriminator(self):

        model = Sequential()

        model.add(Flatten(input_shape=self.img_shape))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Flatten())
        # (!!!) No softmax
        model.add(Dense(1))
        model.summary()

        return model

    def train(self, epochs, BATCH_SIZE = 128):
        """Trains the GAN model. Epoch is number of time that the training set is fed through network in training"""
        (X_train,_), (_,_) = mnist.load_data()
        X_train = X_train.reshape(X_train.shape[0], self.img_rows, self.img_cols, self.channels).astype('float32')
        X_train = (X_train - 127.5) / 127.5

        BUFFER_SIZE = 60000

        train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

        self.losses = np.zeros((epochs*len(train_dataset), 3))

        criterion = tf.keras.losses.MeanSquaredError()

        generator_optimizer = Adam(0.0001)
        discriminator_optimizer = Adam(0.0001)

        def discriminator_loss(real_output, fake_output):
            real_loss = criterion(tf.ones_like(real_output), real_output)
            fake_loss = criterion(tf.zeros_like(fake_output), fake_output)
            total_loss = 0.5 * (real_loss + fake_loss)
            return total_loss

        def generator_loss(fake_output):
            return criterion(tf.ones_like(fake_output), fake_output)

        def discriminator_accuracy(real_output, fake_output):
            preds = np.concatenate([real_output, fake_output])
            target = np.concatenate([np.ones_like(real_output), np.zeros_like(fake_output)])
            preds_int = (preds > 0.5).astype(int)
            target_int = (target > 0.5).astype(int)
            ans = np.sum(preds_int == target_int)/len(preds)
            return ans

        @tf.function
        def train_step(images):
            noise = tf.random.normal([BATCH_SIZE, self.latent_dim])

            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                gen_imgs = self.generator(noise, training = True)

                real_output = self.discriminator(images, training = True)
                fake_output = self.discriminator(gen_imgs, training = True)

                gen_loss = generator_loss(fake_output)
                disc_loss = discriminator_loss(real_output, fake_output)

            grads_gen = gen_tape.gradient(gen_loss, self.generator.trainable_variables)
            grads_disc = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)

            generator_optimizer.apply_gradients(zip(grads_gen, self.generator.trainable_variables))
            discriminator_optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_variables))

            return disc_loss, gen_loss, real_output, fake_output


        for epoch in range(epochs):

            start = time.time()
            batch_size = len(train_dataset)
            batch = 0

            for image_batch in train_dataset:
                self.losses[batch_size*epoch + batch, 0] , self.losses[batch_size*epoch + batch, 1] , real_output, fake_output = train_step(image_batch)
                disc_acc = discriminator_accuracy(real_output.numpy(), fake_output.numpy())
                self.losses[batch_size*epoch + batch, 2] = disc_acc
                print("%d [D loss: %f] [G loss: %f] [D acc: %f]" % (batch_size*epoch + batch, self.losses[batch_size*epoch + batch, 0], self.losses[batch_size*epoch + batch, 1], self.losses[batch_size*epoch + batch, 2]))
                batch = batch + 1


            print("%d [Batch time: %.3f]" % (epoch, time.time()-start))

        return print("Training complete!")


    def save_weights(self, name):

        self.generator.save((name + "_lsgan_generator.h5"))
        self.discriminator.save((name +"_lsgan_discriminator.h5"))
        self.combined.save((name + "_lsgan_combined.h5"))

        return print("Model saved")

    def load_weights(self, name):

        self.generator = load_model((name + "_lsgan_generator.h5"))
        self.discriminator = load_model((name +"_lsgan_discriminator.h5"))
        self.combined = load_model((name + "_lsgan_combined.h5"))

        return print("Model loaded")

    def get_feature_encoder(self):

        transformer = Model(self.discriminator.inputs, self.discriminator.layers[-2].output)

        return transformer

    def save_feature_encoder(self, name):

        encoder = self.get_feature_encoder()
        encoder.save((name + "_lsgan_encoder.h5"))

        return print("Encoder saved")

    def load_feature_encoder(self, name):

        encoder = self.get_feature_encoder()
        encoder = load_model((name + "_lsgan_encoder.h5"))
        encoder.compile()

        return encoder

    def get_training_metrics(self):

        return self.losses, self.disc_acc


class wGanMnist():
    def __init__(self):
        """Initialises necessasry values and objects"""
        # Input shape
        self.img_rows = 28
        self.img_cols = 28
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 100

        # Build and compile the discriminator
        self.discriminator = self.build_critic()

        # Build the generator
        self.generator = self.build_generator()

    def build_generator(self):
        """Builds the generator model of the GAN"""

        model = Sequential()

        model.add(Dense(128 * 7 * 7, activation = "relu", input_dim=self.latent_dim))
        model.add(Reshape((7, 7, 128)))
        model.add(UpSampling2D())
        model.add(Conv2D(128, kernel_size=4, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Activation("relu"))
        model.add(UpSampling2D())
        model.add(Conv2D(64, kernel_size=4, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Activation("relu"))
        model.add(Conv2D(self.channels, kernel_size=4, padding="same"))

        assert model.output_shape == (None, self.img_cols, self.img_rows, self.channels)

        model.summary()

        return model

    def build_critic(self):
        """Builds the critic  model of the GAN"""
        model = Sequential()

        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding="same"))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Conv2D(32, kernel_size=3, strides=2, padding="same"))
        model.add(ZeroPadding2D(padding=((0,1),(0,1))))
        model.add(BatchNormalization(momentum=0.8))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Conv2D(128, kernel_size=3, strides=1, padding="same"))
        model.add(BatchNormalization(momentum=0.8))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dropout(0.25))
        model.add(Flatten())
        model.add(Dense(1))

        model.summary()

        return model

    def train(self, epochs, BATCH_SIZE = 128):
        """Trains the GAN model. Epoch is number of time that the training set is fed through network in training"""
        (X_train,_), (_,_) = mnist.load_data()
        X_train = X_train.reshape(X_train.shape[0], self.img_rows, self.img_cols, self.channels).astype('float32')
        X_train = (X_train - 127.5) / 127.5

        BUFFER_SIZE = 60000

        train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

        self.losses = np.zeros((epochs*len(train_dataset), 3))

        generator_optimizer = RMSprop(lr = 0.00005)
        discriminator_optimizer = RMSprop(lr = 0.00005)

        def discriminator_loss(real_logits, fake_logits):
            total_loss = tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)
            return total_loss

        def generator_loss(fake_logits):
            return -tf.reduce_mean(fake_logits)

        def discriminator_accuracy(real_output, fake_output):
            preds = np.concatenate([real_output, fake_output])
            target = np.concatenate([np.ones_like(real_output), np.zeros_like(fake_output)])
            preds_int = (preds > 0.5).astype(int)
            target_int = (target > 0.5).astype(int)
            ans = np.sum(preds_int == target_int)/len(preds)
            return ans

        @tf.function
        def train_step(images):
            noise = tf.random.normal([BATCH_SIZE, self.latent_dim])

            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                gen_imgs = self.generator(noise, training = True)

                real_output = self.discriminator(images, training = True)
                fake_output = self.discriminator(gen_imgs, training = True)

                gen_loss = generator_loss(fake_output)
                disc_loss = discriminator_loss(real_output, fake_output)

            grads_gen = gen_tape.gradient(gen_loss, self.generator.trainable_variables)
            grads_disc = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)

            generator_optimizer.apply_gradients(zip(grads_gen, self.generator.trainable_variables))
            discriminator_optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_variables))

            return disc_loss, gen_loss, real_output, fake_output


        for epoch in range(epochs):

            start = time.time()
            batch_size = len(train_dataset)
            batch = 0

            for image_batch in train_dataset:
                self.losses[batch_size*epoch + batch, 0] , self.losses[batch_size*epoch + batch, 1], real_output, fake_output = train_step(image_batch)
                disc_acc = discriminator_accuracy(real_output.numpy(), fake_output.numpy())
                self.losses[batch_size*epoch + batch, 2] = disc_acc
                print("%d [D loss: %f] [G loss: %f] [D acc: %f]" % (batch_size*epoch + batch, self.losses[batch_size*epoch + batch, 0], self.losses[batch_size*epoch + batch, 1], self.losses[batch_size*epoch + batch, 2]))
                batch = batch + 1


            print("%d [Batch time: %.3f]" % (epoch, time.time()-start))

        return print("Training complete!")


    def save_weights(self, name):

        self.generator.save((name + "_wgan_generator.h5"))
        self.discriminator.save((name +"_wgan_discriminator.h5"))
        self.combined.save((name + "_wgan_combined.h5"))

        return print("Model saved")

    def load_weights(self, name):

        self.generator.load_model((name + "_wgan_generator.h5"))
        self.discriminator.load_model((name +"_wgan_discriminator.h5"))
        self.combined.load_model((name + "_wgan_combined.h5"))

        return print("Model loaded")

    def get_feature_encoder(self):

        transformer = Model(self.discriminator.inputs, self.discriminator.layers[-2].output)

        return transformer

    def save_feature_encoder(self, name):

        encoder = self.get_feature_encoder()
        encoder.save((name + "_wgan_encoder.h5"))

        return print("Encoder saved")

    def load_feature_encoder(self, name):

        encoder = self.get_feature_encoder()
        encoder = load_model((name + "_wgan_encoder.h5"))
        encoder.compile()

        return encoder


